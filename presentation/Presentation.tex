\documentclass[10pt, reqno]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1cm, a6paper, landscape]{geometry}
\usepackage{amsmath, amsfonts, amssymb, amsthm, graphicx}
\usepackage[ruled]{algorithm2e}
\usepackage{caption}

\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\hypersetup{colorlinks, linkcolor={MidnightBlue}, citecolor={MidnightBlue}, urlcolor={MidnightBlue}}

\usepackage{tikz}

\setlength{\parindent}{0pt}
\newcommand{\forceindent}{\leavevmode{\parindent=1em\indent}}
\numberwithin{equation}{section}

\newtheorem*{theorem*}{Theorem}
\newtheorem*{cor*}{Corollary}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\e}{\epsilon}
\newcommand{\w}{\omega}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\ae}[1]{\text{ \hspace{5pt} a.e. #1}}

\begin{document}
%\pagestyle{empty}
\setlength{\footskip}{-1cm}
 

\title{Combining $l^1$ Penalization with Higher Moment Constraints in Regression Models}
\author{Austin David Brown}
\date{\today}

%%%
% Slide
%%%
\maketitle

%%%
% Slide
%%%
\newpage
\section*{Motivation}

\begin{itemize}
\item In statistics and probability theory it is common to impose moment assumptions on a random variable $X : \Omega \to \R^n$ such as $E(\norm{X}^k) < \infty$ for $k \in \R$.

\item These constraints correspond to the $L^p$ spaces which allow control over the width and the height of such random variables.
This can be interpreted as imposing "stability" on the random variable $X$.

\item If statisticians so freely impose such constraints then we should build a tool to allow scientists and researchers to impose such constraints on their real problems.

\item  In this project, we build a package implements this idea.
The goal is to build upon the "stability" property of ElasticNet \cite{elasticnet} and create a tool available to scientists and researchers.

\end{itemize}

%%%
% Slide
%%%
\newpage
\section*{Geometric Motivation}
Consider for example an Elasticnet \cite{elasticnet} penalty $Q(x) = \frac{1}{2} |x| + \frac{1}{2} |y| + \frac{1}{2} |x|^2 + \frac{1}{2} |y|^2 \le 1$
shown on the left. Extending this idea, a new penalty $P(x) = \frac{1}{2} |x| + \frac{1}{2} |y| + \frac{1}{2} |x|^4 + \frac{1}{2} |y|^4 \le 1$ shown on the right.

\vspace{.5cm}
\begin{center}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{elasticnet.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{new_penalty_4_moment.png}
\end{minipage}
\end{center}
\vspace{.5cm}

It seems reasonable that a scientist or researcher may want the option to "bow" out the feasible set even more.

%%%
% Slide
%%%
\newpage
\section*{The Setup}

Define a new penalty to try to impose more "stability" for scientists and researchers (try to extend the Elasticnet idea). Let
\[
L_{\lambda}(\beta) = \frac{1}{2} \norm{y - X \beta}_2^2 + \lambda P(\beta)
\]

with penalty

\[
\lambda P(\beta) = \lambda \alpha_0 \norm{\beta}_1 + \lambda \sum_{k = 1}^{5} \alpha_k \norm{\beta}_{2k}^{2k} 
\]

where $y \in \R^n$, $X \in M_{n \times p}(\R)$, and $\beta \in \R^p$, $\lambda \in \R_{+}$ and $\alpha$'s are convex combinations or use separate tuning parameters instead.

\vspace{1cm}
This is a convex, separable penalty separable, but the derivative is not Lipschitz.

%%%
% Slide
%%%
\newpage
\section*{Algorithm Implementation 1}

\vspace{.5cm}
\begin{algorithm}[H]
\caption{Subgradient Coordinate Method}
Choose $\beta^0 \in \R^p$ and tolerance $\delta > 0$;

Set $k \gets 0$

\Repeat{Until the loss difference $\Delta L$ is less than $\delta$}{ 

  Set the step size $h^k \gets \frac{R}{\sqrt{1 + k}}$ for some $R > 0$ or use a constant step size.

  Permute $I = \{1, \ldots, p\}$

  \For {$i \in I$}{

    $\beta^{k + 1}_i \gets \beta^{k}_i - h^i g^i$ where $g^i \in (\partial L)_i$

  }

  $k \gets k + 1$
}

\end{algorithm}

%%%
% Slide
%%%
\newpage
\section*{Drawbacks}

\begin{itemize}
\item Not a descent method.

\item No good stopping criterion.

\item Convergence theory is worse.

\item Tends to produce really small values instead of truly sparse solutions.

\end{itemize}

%%%
% Slide
%%%
\newpage
\section*{A Much Better Algorithm}

\vspace{.5cm}
\begin{algorithm}[H]
\caption{Proximal Gradient Coordinate Descent}
Choose $\beta^0 \in \R^p$ and tolerance $\delta > 0$;

Set $k \gets 0$

\Repeat{Until the Moreau-Yoshida mapping $M_{h_k, f} < \delta$}{ 

  Set the step size $h^k > 0$ with diminishing or line search.

  Randomly permute $I = \{1, \ldots, p\}$

  \For {$i \in I$}{

    $\beta^{k + 1}_i \gets (\textbf{prox}_{h^k L})_i ( \beta^k_i - h^k \langle X_i, y - X \beta \rangle )$

  }

  $k \gets k + 1$
}
\end{algorithm}

%%%
% Slide
%%%
\newpage
\section*{Benefits}

\begin{itemize}
\item A descent method, so can do line search.

\item Good stopping criterion at $O(p^2)$ flops.

\item Convergence theory is unchanged from differentiable functions.

\end{itemize}

%%%
% Slide
%%%
\newpage
\section*{Cross-Validation Implementation}

\vspace{.5cm}
\begin{algorithm}[H]
\caption{Warm Start Cross-Validation}
Choose a sequence of Langrangian dual variables $\lambda_1, \ldots, \lambda_N$, and initial value $\beta^0$.

Order $\lambda_{(1)}, \ldots, \lambda_{(N)}$ descending.

$\beta^{Warm} \gets \beta^0$ 

\For{$k \in 1, \ldots, N$}{

  $\beta^k \gets$ by Cross-Validation with $\lambda_{(k)}$ warm started with $\beta^{Warm}$.

  $\beta^{Warm} \gets \beta^k$
}

\end{algorithm}

%%%
% Slide
%%%
\newpage
\section*{Package Architecture}

Written entirely in C++ for speed using Eigen \cite{eigen} and can be interfaced to many other popular languages.

\vspace{.5cm}
\begin{tikzpicture}[level distance=8em, sibling distance=10em,
  every node/.style = {shape=rectangle, draw, align=center}]]
  \node {PROS C++ code}
    child { node {R to C Interface} 
      child { node {R Interface} }
    }
    child { node {Python Interface} 
      child { node {$\ldots$} }
    }
    child { node {MatLab Interface} 
      child { node {$\ldots$} }
    }
    child { node {Julia Interface} 
      child { node {$\ldots$} }
    };
\end{tikzpicture}

%%%
% Slide 8
%%%
\newpage
\section*{Penalized Regression on Steroids}

Installation: 

\begin{verbatim}
> devtools::install_github("austindavidbrown/pros/R-package")
\end{verbatim}


A single fit function with prediction

\begin{verbatim}
> fit <- pros(X, y, alpha, lambda)
> predict(fit, new_X)
\end{verbatim}

A cross-validation fit function with prediction

\begin{verbatim}
> cv <- cv.pros(X, y, alpha)
> predict(cv, new_X)
\end{verbatim}

There is even a \href{https://github.com/austindavidbrown/pros/blob/master/R-package/pros.pdf}{reference manual}.


%%%
% Slide
%%%
\newpage
\section*{The Prostate Cancer Dataset Analysis}

This is a popular dataset from Stamey et al. \cite{prostate} and analyzed in the ElasticNet paper Zou and Hastie \cite{elasticnet}.
There are 8 predictors and 1 response is the log of the prostate specific antigen.

\begin{itemize}
\item The data is split into a training set with 67 observations and a test set with 30 observations.

\item The predictor data was standardized.

\item 10-fold cross-validation and manual tuning due to time constraint of the project were used.

\item The \textbf{glmnet} \cite{glmnet} library was used to fit and tune the Lasso and a naive un-tuned ElasticNet.

\item \textbf{pros} \cite{pros} was used to fit the new penalty.

\item This was not an extensive study due to time.

\end{itemize}

%%%
% Slide
%%%
\newpage
\section*{The Prostate Cancer Dataset Results}

\begin{center}
\begingroup
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{2} % Default value: 1
\begin{tabular}{ l l l}
\textbf{Penalty} & \textbf{Tuning} & \textbf{Test MSE} \\
Lasso Penalty (glmnet) & $\lambda = 0.001572009$ & 0.4558642 \\
ElasticNet Penalty (glmnet) & $\alpha = (1/2, 1/2)$, $\lambda = 0.01677868$  & 0.4539746 \\
New Penalty (pros) & $\alpha = (1/3, 0, 0, 0, 0, 2/3, 0)$, $\lambda = 20$ & 0.4443996 \\
New Penalty (pros) & $\alpha = (1/5, 0, 1/5, 1/5, 1/5, 1/5, 0)$, $\lambda = 50$ & 0.4479282 \\
\end{tabular}
\endgroup
\end{center}

%%%
% Slide
%%%
\newpage
\section*{Conclusion}

\begin{itemize}
\item I would like to explore the remaining penalizations.

\[
\lambda P'(\beta) = \lambda \alpha_1 \norm{\beta}_1 + \lambda \alpha_2 \norm{\beta}_\infty 
\]

\[
\lambda P''(\beta) = \lambda \alpha_1 \norm{\beta}_1 + \lambda \sum_{k = 2}^{10} \alpha_k \norm{\beta}_{k}
\]


\item Step size issue i.e. Find a fast line search.

\item Look into convergence analysis.

\item Most importantly, I think that we can bring new methods that may be useful to scientists and researchers and we can further study their properties.
\end{itemize}

%
% Bib
%
\newpage
\small
\begin{thebibliography}{1}

\bibitem{prostate}
Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989)
Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients, Journal of Urology 141(5), 1076–1083.

\bibitem{pros}
PROS. \href{https://github.com/austindavidbrown/pros}{github.com/austindavidbrown/pros}

\bibitem{boyd_proximalalgs}
Neal Parikh and Stephen Boyd. 2014. Proximal Algorithms. Found. Trends Optim. 1, 3 (January 2014), 127-239. DOI=10.1561/2400000003 http://dx.doi.org/10.1561/2400000003

\bibitem{wright_cd_algs}
Stephen J. Wright. 2015. Coordinate descent algorithms. Math. Program. 151, 1 (June 2015), 3-34. DOI=10.1007/s10107-015-0892-3 http://dx.doi.org/10.1007/s10107-015-0892-3

\bibitem{eigen}
Guennebaud, Gaël (2013). Eigen: A C++ linear algebra library (PDF). Eurographics/CGLibs.

\bibitem{glmnet}
Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.

\bibitem{elasticnet}
Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B, 67, 301–320.

\bibitem{nesterov}
Yurii Nesterov. 2014. Introductory Lectures on Convex Optimization: A Basic Course (1 ed.). Springer Publishing Company, Incorporated.

\end{thebibliography}

\end{document}

